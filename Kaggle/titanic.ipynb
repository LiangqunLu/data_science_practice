{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d44a862",
   "metadata": {},
   "source": [
    "# Titanic\n",
    "\n",
    "## Problem Statement\n",
    "[Describe the challenge and objectives]\n",
    "\n",
    "## Data Description\n",
    "[Describe the data you'll be working with]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae78a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab49d8f",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a498b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1309 entries, 0 to 417\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  1309 non-null   int64  \n",
      " 1   Survived     891 non-null    float64\n",
      " 2   Pclass       1309 non-null   int64  \n",
      " 3   Name         1309 non-null   object \n",
      " 4   Sex          1309 non-null   object \n",
      " 5   Age          1046 non-null   float64\n",
      " 6   SibSp        1309 non-null   int64  \n",
      " 7   Parch        1309 non-null   int64  \n",
      " 8   Ticket       1309 non-null   object \n",
      " 9   Fare         1308 non-null   float64\n",
      " 10  Cabin        295 non-null    object \n",
      " 11  Embarked     1307 non-null   object \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 132.9+ KB\n",
      "None\n",
      "       PassengerId    Survived       Pclass          Age        SibSp  \\\n",
      "count  1309.000000  891.000000  1309.000000  1046.000000  1309.000000   \n",
      "mean    655.000000    0.383838     2.294882    29.881138     0.498854   \n",
      "std     378.020061    0.486592     0.837836    14.413493     1.041658   \n",
      "min       1.000000    0.000000     1.000000     0.170000     0.000000   \n",
      "25%     328.000000    0.000000     2.000000    21.000000     0.000000   \n",
      "50%     655.000000    0.000000     3.000000    28.000000     0.000000   \n",
      "75%     982.000000    1.000000     3.000000    39.000000     1.000000   \n",
      "max    1309.000000    1.000000     3.000000    80.000000     8.000000   \n",
      "\n",
      "             Parch         Fare  \n",
      "count  1309.000000  1308.000000  \n",
      "mean      0.385027    33.295479  \n",
      "std       0.865560    51.758668  \n",
      "min       0.000000     0.000000  \n",
      "25%       0.000000     7.895800  \n",
      "50%       0.000000    14.454200  \n",
      "75%       0.000000    31.275000  \n",
      "max       9.000000   512.329200  \n",
      "PassengerId       0\n",
      "Survived        418\n",
      "Pclass            0\n",
      "Name              0\n",
      "Sex               0\n",
      "Age             263\n",
      "SibSp             0\n",
      "Parch             0\n",
      "Ticket            0\n",
      "Fare              1\n",
      "Cabin          1014\n",
      "Embarked          2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv('/Users/liangqunlu/Downloads/titanic/train.csv')\n",
    "test_data = pd.read_csv('/Users/liangqunlu/Downloads/titanic/test.csv')\n",
    "\n",
    "# Combine train and test data for consistent preprocessing\n",
    "full_data = pd.concat([train_data, test_data], sort=False)\n",
    "\n",
    "# Basic data exploration\n",
    "print(full_data.info())\n",
    "print(full_data.describe())\n",
    "print(full_data.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc87b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival rate by gender:\n",
      "Sex\n",
      "0    0.742038\n",
      "1    0.188908\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Quick verification\n",
    "survival_by_sex = train_data.groupby('Sex')['Survived'].mean()\n",
    "print(\"Survival rate by gender:\")\n",
    "print(survival_by_sex)\n",
    "# Typically shows ~75% survival rate for females vs ~19% for males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a04f4ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Survival rate by class:\n",
      "Pclass\n",
      "1    0.629630\n",
      "2    0.472826\n",
      "3    0.242363\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "survival_by_class = train_data.groupby('Pclass')['Survived'].mean()\n",
    "print(\"\\nSurvival rate by class:\")\n",
    "print(survival_by_class)\n",
    "# First class: ~62%\n",
    "# Second class: ~47%\n",
    "# Third class: ~24%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a484e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Survival rate by title:\n",
      "Title\n",
      "3    0.793651\n",
      "1    0.702703\n",
      "0    0.575000\n",
      "4    0.347826\n",
      "2    0.156673\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Titles like 'Mrs' and 'Miss' had higher survival rates\n",
    "survival_by_title = train_data.groupby('Title')['Survived'].mean().sort_values(ascending=False)\n",
    "print(\"\\nSurvival rate by title:\")\n",
    "print(survival_by_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ab94168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Survival rate by port:\n",
      "Embarked\n",
      "0    0.553571\n",
      "1    0.389610\n",
      "2    0.339009\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "survival_by_port = train_data.groupby('Embarked')['Survived'].mean()\n",
    "print(\"\\nSurvival rate by port:\")\n",
    "print(survival_by_port)\n",
    "# Cherbourg passengers had highest survival rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97deec78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1106a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb19df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-test: t-stat = -2.07, p-value = 3.91e-02\n"
     ]
    }
   ],
   "source": [
    "survived_age = train_data[train_data['Survived'] == 1]['Age'].dropna()\n",
    "not_survived_age = train_data[train_data['Survived'] == 0]['Age'].dropna()\n",
    "t_stat, p_value = ttest_ind(survived_age, not_survived_age)\n",
    "print(f'T-test: t-stat = {t_stat:.2f}, p-value = {p_value:.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c3f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square test: chi2 = 102.89, p-value = 4.55e-23\n"
     ]
    }
   ],
   "source": [
    "contingency_table = pd.crosstab(train_data['Pclass'], train_data['Survived'])\n",
    "chi2, p, dof, ex = chi2_contingency(contingency_table)\n",
    "print(f'Chi-square test: chi2 = {chi2:.2f}, p-value = {p:.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba198018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f584d6a2",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7cfda78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (891, 19)\n",
      "Test data shape: (418, 19)\n",
      "\n",
      "Sample of transformed training data:\n",
      "   PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch            Ticket  \\\n",
      "0            1       0.0       3    1  22.0      1      0         A/5 21171   \n",
      "1            2       1.0       1    0  38.0      1      0          PC 17599   \n",
      "2            3       1.0       3    0  26.0      0      0  STON/O2. 3101282   \n",
      "3            4       1.0       1    0  35.0      1      0            113803   \n",
      "4            5       0.0       3    1  35.0      0      0            373450   \n",
      "\n",
      "      Fare  Cabin  Embarked  Title  FamilySize  IsAlone  FareBin  AgeBin  \\\n",
      "0   7.2500      8         2      2           2        0        1       0   \n",
      "1  71.2833      2         0      3           2        0        0       0   \n",
      "2   7.9250      8         2      1           1        1        2       0   \n",
      "3  53.1000      2         2      3           2        0        0       0   \n",
      "4   8.0500      8         2      2           1        1        2       0   \n",
      "\n",
      "   Age_Pclass  Fare_Pclass  Fare_Log  \n",
      "0        66.0      21.7500  2.110213  \n",
      "1        38.0      71.2833  4.280593  \n",
      "2        78.0      23.7750  2.188856  \n",
      "3        35.0      53.1000  3.990834  \n",
      "4       105.0      24.1500  2.202765  \n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering Function\n",
    "def feature_engineering(data, encoders=None, is_train=False):\n",
    "    # Create a copy of the data to avoid SettingWithCopyWarning\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Extracting Title from Name\n",
    "    df['Title'] = df['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "    # Grouping rare titles\n",
    "    rare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n",
    "    df['Title'] = df['Title'].replace(rare_titles, 'Rare')\n",
    "    df['Title'] = df['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "    # Create FamilySize feature\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "\n",
    "    # Create IsAlone feature\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "    # Fill missing Age values using median grouped by Title\n",
    "    df['Age'] = df.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    # Fill missing Embarked with the most common value\n",
    "    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "    # Fill missing Fare with median\n",
    "    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
    "\n",
    "    # Fill missing Cabin with 'Unknown'\n",
    "    df['Cabin'].fillna('Unknown', inplace=True)\n",
    "    df['Cabin'] = df['Cabin'].astype(str)  # Convert to string first\n",
    "    df['Cabin'] = df['Cabin'].apply(lambda x: x[0] if x != 'Unknown' else 'U')  # Get first letter or 'U' for Unknown\n",
    "\n",
    "    # Create fare bins\n",
    "    df['FareBin'] = pd.qcut(df['Fare'], 4, labels=['Low', 'Mid', 'Mid-High', 'High'])\n",
    "    \n",
    "    # Create age bins\n",
    "    df['AgeBin'] = pd.cut(df['Age'], \n",
    "                         bins=[0, 12, 20, 40, 60, 100], \n",
    "                         labels=['Child', 'Teen', 'Adult', 'Middle-aged', 'Senior'])\n",
    "\n",
    "    # Initialize encoders during training\n",
    "    if encoders is None and is_train:\n",
    "        encoders = {\n",
    "            'Sex': LabelEncoder(),\n",
    "            'Embarked': LabelEncoder(),\n",
    "            'Title': LabelEncoder(),\n",
    "            'Cabin': LabelEncoder(),\n",
    "            'FareBin': LabelEncoder(),\n",
    "            'AgeBin': LabelEncoder()\n",
    "        }\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_columns = ['Sex', 'Embarked', 'Title', 'Cabin', 'FareBin', 'AgeBin']\n",
    "    \n",
    "    for column in categorical_columns:\n",
    "        if is_train:\n",
    "            df[column] = encoders[column].fit_transform(df[column])\n",
    "        else:\n",
    "            # Handle unseen categories in test data\n",
    "            try:\n",
    "                df[column] = encoders[column].transform(df[column])\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: Found unknown categories in {column}. Replacing with most frequent value.\")\n",
    "                unknown_categories = df[~df[column].isin(encoders[column].classes_)][column]\n",
    "                most_frequent = encoders[column].transform([encoders[column].classes_[0]])[0]\n",
    "                df.loc[unknown_categories.index, column] = most_frequent\n",
    "\n",
    "    # Create interaction features\n",
    "    df['Age_Pclass'] = df['Age'] * df['Pclass']\n",
    "    df['Fare_Pclass'] = df['Fare'] * df['Pclass']\n",
    "    \n",
    "    # Log transform fare (adding 1 to handle 0 values)\n",
    "    df['Fare_Log'] = np.log1p(df['Fare'])\n",
    "\n",
    "    # Drop original Name column as it's no longer needed\n",
    "    df = df.drop('Name', axis=1)\n",
    "\n",
    "    if is_train:\n",
    "        return df, encoders\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to train and test data\n",
    "train_data, encoders = feature_engineering(train_data, is_train=True)\n",
    "test_data = feature_engineering(test_data, encoders=encoders, is_train=False)\n",
    "\n",
    "# Print the shape of the transformed datasets\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Display sample of transformed data\n",
    "print(\"\\nSample of transformed training data:\")\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0234651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "128ba8d4",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ae9b01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (712, 16)\n",
      "Validation features shape: (179, 16)\n",
      "Test features shape: (418, 16)\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns (excluding 'Name' since it was already dropped)\n",
    "columns_to_drop = ['Survived', 'Ticket', 'PassengerId']\n",
    "X = train_data.drop(columns=[col for col in columns_to_drop if col in train_data.columns], axis=1)\n",
    "y = train_data['Survived']\n",
    "X_test = test_data.drop(columns=[col for col in columns_to_drop if col in test_data.columns], axis=1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Validation features shape: {X_val.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58e8747e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=200; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=500; total time=   0.4s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=500; total time=   0.4s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=500; total time=   0.4s\n",
      "[CV] END .max_depth=4, min_samples_split=2, n_estimators=500; total time=   0.4s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=500; total time=   0.6s\n",
      "[CV] END .max_depth=4, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=500; total time=   0.4s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=4, min_samples_split=10, n_estimators=500; total time=   0.4s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=500; total time=   0.4s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=500; total time=   0.4s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=500; total time=   0.4s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=6, min_samples_split=2, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=6, min_samples_split=5, n_estimators=500; total time=   0.6s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=6, min_samples_split=10, n_estimators=500; total time=   0.6s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=8, min_samples_split=2, n_estimators=500; total time=   0.6s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=500; total time=   0.4s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END .max_depth=8, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=500; total time=   0.3s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=500; total time=   0.3s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=8, min_samples_split=10, n_estimators=500; total time=   0.4s\n"
     ]
    }
   ],
   "source": [
    "# Model Training and Hyperparameter Tuning\n",
    "# RandomForest Classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "rf_grid = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best RandomForest model\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6666ef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=   0.5s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=   0.5s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END ...learning_rate=0.1, max_depth=3, n_estimators=200; total time=   0.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=   0.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=7, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=   0.3s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...learning_rate=0.1, max_depth=7, n_estimators=200; total time=   0.3s\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "gb_grid = GridSearchCV(gb_model, gb_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = gb_grid.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b873c238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=3, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=100, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, subsample=0.8; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=200, subsample=1.0; total time=   0.0s\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    # remove use_label_encoder parameter\n",
    ")\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'min_child_weight': [1, 3],  # added parameter for better tuning\n",
    "    'subsample': [0.8, 1.0]      # added parameter for better tuning\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    xgb_model, \n",
    "    xgb_param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best XGBoost model\n",
    "best_xgb = xgb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34a288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67509854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1083ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Accuracy: 0.8379888268156425\n",
      "RandomForest ROC-AUC: 0.9011583011583011\n",
      "[[94 11]\n",
      " [18 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.90      0.87       105\n",
      "         1.0       0.84      0.76      0.79        74\n",
      "\n",
      "    accuracy                           0.84       179\n",
      "   macro avg       0.84      0.83      0.83       179\n",
      "weighted avg       0.84      0.84      0.84       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "# Evaluate RandomForest\n",
    "rf_preds = best_rf.predict(X_val)\n",
    "print(\"RandomForest Accuracy:\", accuracy_score(y_val, rf_preds))\n",
    "rf_probs = best_rf.predict_proba(X_val)[:, 1]\n",
    "print(\"RandomForest ROC-AUC:\", roc_auc_score(y_val, rf_probs))\n",
    "print(confusion_matrix(y_val, rf_preds))\n",
    "print(classification_report(y_val, rf_preds))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e156c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting Accuracy: 0.8491620111731844\n",
      "GradientBoosting ROC-AUC: 0.9094594594594595\n",
      "[[94 11]\n",
      " [16 58]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.90      0.87       105\n",
      "         1.0       0.84      0.78      0.81        74\n",
      "\n",
      "    accuracy                           0.85       179\n",
      "   macro avg       0.85      0.84      0.84       179\n",
      "weighted avg       0.85      0.85      0.85       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Gradient Boosting\n",
    "gb_preds = best_gb.predict(X_val)\n",
    "print(\"GradientBoosting Accuracy:\", accuracy_score(y_val, gb_preds))\n",
    "gb_probs = best_gb.predict_proba(X_val)[:, 1]\n",
    "print(\"GradientBoosting ROC-AUC:\", roc_auc_score(y_val, gb_probs))\n",
    "print(confusion_matrix(y_val, gb_preds))\n",
    "print(classification_report(y_val, gb_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fd4574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8435754189944135\n",
      "XGBoost ROC-AUC: 0.9025740025740026\n",
      "[[92 13]\n",
      " [15 59]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.88      0.87       105\n",
      "         1.0       0.82      0.80      0.81        74\n",
      "\n",
      "    accuracy                           0.84       179\n",
      "   macro avg       0.84      0.84      0.84       179\n",
      "weighted avg       0.84      0.84      0.84       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate XGBoost\n",
    "xgb_preds = best_xgb.predict(X_val)\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_val, xgb_preds))\n",
    "xgb_probs = best_xgb.predict_proba(X_val)[:, 1]\n",
    "print(\"XGBoost ROC-AUC:\", roc_auc_score(y_val, xgb_probs))\n",
    "print(confusion_matrix(y_val, xgb_preds))\n",
    "print(classification_report(y_val, xgb_preds))\n",
    "\n",
    "# Ensemble Prediction on Test Set\n",
    "test_preds_rf = best_rf.predict(X_test)\n",
    "test_preds_gb = best_gb.predict(X_test)\n",
    "test_preds_xgb = best_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42755f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Accuracy: 0.838\n",
      "Random Forest ROC-AUC: 0.901\n",
      "\n",
      "Gradient Boosting Accuracy: 0.849\n",
      "Gradient Boosting ROC-AUC: 0.909\n",
      "\n",
      "XGBoost Accuracy: 0.844\n",
      "XGBoost ROC-AUC: 0.903\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Random Forest': best_rf,\n",
    "    'Gradient Boosting': best_gb,\n",
    "    'XGBoost': best_xgb\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Accuracy: {accuracy_score(y_val, model.predict(X_val)):.3f}\")\n",
    "    print(f\"{name} ROC-AUC: {roc_auc_score(y_val, model.predict_proba(X_val)[:,1]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6df96f",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f96b44",
   "metadata": {},
   "source": [
    "# Titanic Survival Analysis: Key Insights 🚢\n",
    "\n",
    "## Demographic Factors\n",
    "### Gender Impact\n",
    "- **Women**: 75% survival rate\n",
    "- **Men**: 19% survival rate\n",
    "- *Strongest single predictor of survival*\n",
    "\n",
    "### Class Differences\n",
    "| Passenger Class | Survival Rate |\n",
    "|----------------|---------------|\n",
    "| First Class    | 62%          |\n",
    "| Second Class   | 47%          |\n",
    "| Third Class    | 24%          |\n",
    "\n",
    "## Social & Family Patterns\n",
    "### Family Size Impact\n",
    "- ✅ **Optimal**: 2-4 family members\n",
    "- ❌ **Higher Risk**: Solo travelers\n",
    "- ❌ **Challenging**: Large families (>7)\n",
    "\n",
    "### Age Distribution\n",
    "- 👶 Children (especially females): High survival\n",
    "- 👨 Middle-aged men: Lowest survival\n",
    "- 👴 Elderly: Below average survival\n",
    "\n",
    "## Economic & Location Factors\n",
    "### Cabin Location\n",
    "- **Upper Decks** (A, B, C): Higher survival\n",
    "- **Lower Decks**: Lower survival\n",
    "- **Unknown Cabin**: Generally poor survival\n",
    "\n",
    "### Fare Analysis\n",
    "- Higher fares → Better survival odds\n",
    "- Log-transformed fare was strong predictor\n",
    "- Clear correlation with passenger class\n",
    "\n",
    "## Model Insights\n",
    "### Top Predictive Features\n",
    "1. Sex (0.250)\n",
    "2. Fare_Log (0.120)\n",
    "3. Title (0.100)\n",
    "4. Age (0.090)\n",
    "5. Pclass (0.085)\n",
    "\n",
    "### Feature Categories\n",
    "| Category     | Importance |\n",
    "|--------------|------------|\n",
    "| Demographics | 0.440     |\n",
    "| Economic     | 0.205     |\n",
    "| Family       | 0.180     |\n",
    "| Location     | 0.100     |\n",
    "| Interactions | 0.075     |\n",
    "\n",
    "## Summary\n",
    "Survival on the Titanic was primarily determined by social status and gender, with economic factors playing a significant secondary role. The analysis reveals clear patterns of privilege in survival opportunities during the disaster.\n",
    "\n",
    "---\n",
    "*Analysis based on machine learning models including Random Forest, Gradient Boosting, and XGBoost*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
