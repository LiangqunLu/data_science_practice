{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text Classification\n",
    "You can use the pipeline function for quick experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")\n",
    "result = classifier(\"I love using Hugging Face Transformers!\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give you a quick result on the sentiment of the text.\n",
    "\n",
    "To fine-tune a text classification model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset and model\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Fine-tuning\n",
    "training_args = TrainingArguments(output_dir=\"./results\", evaluation_strategy=\"epoch\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Named Entity Recognition (NER)\n",
    "For Named Entity Recognition, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "result = ner(\"Hugging Face Inc. is a company based in New York City.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Text Generation\n",
    "To generate text using GPT-2:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "result = generator(\"Once upon a time\", max_length=50, num_return_sequences=1)\n",
    "print(result)\n",
    "\n",
    "# If you want to fine-tune GPT-2, you can use:\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Text Summarization\n",
    "For text summarization, use BART or T5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "text = \"\"\"\n",
    "Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\n",
    "Hugging Face is known for its open-source library Transformers, which provides pre-trained models for natural language processing.\n",
    "\"\"\"\n",
    "result = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Question Answering\n",
    "For question answering, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "result = question_answerer(question=\"Where is Hugging Face based?\", context=\"Hugging Face Inc. is based in New York City.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Translation Example\n",
    "For translation, you can use the mBART model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "result = translator(\"Hello, how are you?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 迁移学习 (Transfer Learning)\n",
    "\n",
    "迁移学习是利用在大型数据集上训练的预训练模型（如 BERT、GPT-2），然后对特定任务进行微调。示例代码如上所示，BERT 微调情感分析任务即为迁移学习的典型应用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 零样本学习 (Zero-Shot Learning)\n",
    "\n",
    "使用零样本分类模型（例如 BART 或 RoBERTa）进行分类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "sequence_to_classify = \"This is a great product, I love it!\"\n",
    "candidate_labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "result = classifier(sequence_to_classify, candidate_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 少量样本学习 (Few-Shot Learning)\n",
    "\n",
    "利用 GPT-3 或类似的大型语言模型，通过提供几个示例进行少量样本学习："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "prompt = \"\"\"\n",
    "The following are examples of classifying movie reviews as positive or negative:\n",
    "\n",
    "Review: \"This movie was amazing, the acting was great!\"\n",
    "Label: Positive\n",
    "\n",
    "Review: \"The film was dull and boring.\"\n",
    "Label: Negative\n",
    "\n",
    "Review: \"I had a great time watching it.\"\n",
    "Label:\n",
    "\"\"\"\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=5\n",
    ")\n",
    "\n",
    "print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 从0训练模型 (Training from Scratch)\n",
    "\n",
    "从头开始训练一个 Transformer 模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 定义模型配置\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification(config)\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 对数据进行tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(output_dir=\"./results\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "# 使用 Trainer 进行训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有难度的类型\n",
    "\n",
    "除了上面提到的常见任务，NLP 中还有一些更具挑战性的任务：\n",
    "\n",
    "多任务学习 (Multi-Task Learning)：训练一个模型同时完成多个不同类型的任务，例如同时进行文本分类和命名实体识别。\n",
    "\n",
    "领域自适应 (Domain Adaptation)：将模型从一个领域迁移到另一个领域，通常需要应对不同领域之间的差异，例如从新闻数据迁移到医学数据。\n",
    "\n",
    "强化学习 (Reinforcement Learning for NLP)：在对话生成或摘要中使用强化学习来优化生成质量。\n",
    "\n",
    "元学习 (Meta Learning)：通过学习如何学习，提高模型在少样本场景中的表现。\n",
    "\n",
    "对抗性训练 (Adversarial Training)：训练模型在面对对抗性输入时具有更好的鲁棒性，防止模型受到对抗性攻击的影响。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
